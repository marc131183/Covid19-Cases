{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "plt.rcParams[\"figure.figsize\"] = (24, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get currently available data\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/owid/covid-19-data/master/public/data/owid-covid-data.csv\",\n",
    "    parse_dates=[\"date\"],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1077,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select attributes that we find interesting\n",
    "attributes = [\n",
    "    \"iso_code\",\n",
    "    \"continent\",\n",
    "    \"location\",\n",
    "    \"date\",\n",
    "    \"total_cases\",\n",
    "    \"new_cases\",\n",
    "    \"total_deaths\",\n",
    "    \"new_deaths\",\n",
    "    \"reproduction_rate\",\n",
    "    \"icu_patients\",\n",
    "    \"hosp_patients\",\n",
    "    \"weekly_icu_admissions\",\n",
    "    \"weekly_hosp_admissions\",\n",
    "    \"new_tests\",\n",
    "    \"total_tests\",\n",
    "    \"positive_rate\",\n",
    "    \"total_vaccinations\",\n",
    "    \"people_vaccinated\",\n",
    "    \"people_fully_vaccinated\",\n",
    "    \"new_vaccinations\",\n",
    "    \"stringency_index\",\n",
    "    \"population\",\n",
    "    \"population_density\",\n",
    "    \"median_age\",\n",
    "    \"aged_65_older\",\n",
    "    \"aged_70_older\",\n",
    "    \"gdp_per_capita\",\n",
    "    \"extreme_poverty\",\n",
    "    \"cardiovasc_death_rate\",\n",
    "    \"diabetes_prevalence\",\n",
    "    \"female_smokers\",\n",
    "    \"male_smokers\",\n",
    "    \"handwashing_facilities\",\n",
    "    \"hospital_beds_per_thousand\",\n",
    "    \"life_expectancy\",\n",
    "    \"human_development_index\",\n",
    "    \"excess_mortality\",\n",
    "]\n",
    "\n",
    "df = df[attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1078,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check which columns contain NaN values\n",
    "df.isna().any().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's weird that continent contains NaNs and location doesn't, so let's take a look at that\n",
    "print(\"First 5 rows that have NaN for the column continent\")\n",
    "df[df[\"continent\"].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1080,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's store these rows which combine data of a continent in a seperate dataframe, so it doesn't give us any weird mistakes later/confuse us\n",
    "# before we do this we should be sure that really only these combined rows have NaNs\n",
    "print(\n",
    "    \"Unique locations where continent is NaN\",\n",
    "    df[df[\"continent\"].isna()][\"location\"].unique(),\n",
    ")\n",
    "# looking good, so lets create a new dataframe\n",
    "continent_df = df[df[\"continent\"].isna()]\n",
    "# drop these rows from the original dataframe\n",
    "df = df[~df[\"continent\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we would expect that new_cases should be almost complete (because it's the most important attribute), so let's take a look at that\n",
    "print(\"Percentage of non-missing values for each country in the column new_cases:\")\n",
    "(\n",
    "    df[[\"location\", \"new_cases\"]].groupby(\"location\").count()[\"new_cases\"]\n",
    "    / df[[\"location\", \"new_cases\"]].groupby(\"location\").size()\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that there are countries that don't have any values for new_cases or simply not enough values, these are obviously useless to us\n",
    "# lets drop all countries that have missing values for more than half of their entries for the column new_cases\n",
    "# (if they have more than half of the values, then we can fix the missing values with interpolation later)\n",
    "temp = (\n",
    "    df[[\"location\", \"new_cases\"]].groupby(\"location\").count()[\"new_cases\"]\n",
    "    / df[[\"location\", \"new_cases\"]].groupby(\"location\").size()\n",
    ") >= 0.5\n",
    "temp = temp[temp == True].index.tolist()\n",
    "# drop all rows that don't fulfill the above defined criteria\n",
    "df = df[df[\"location\"].isin(temp)]\n",
    "# reset index, so that it is correct again (we dropped rows)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1083,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also while looking through the records, we found that some countries have leading NaNs for new_cases\n",
    "# let's remove these (while we're at it, let's also remove trailing NaNs)\n",
    "df = df.sort_values(by=[\"location\", \"date\"]).reset_index(drop=True)\n",
    "# get the first and last valid index\n",
    "first_valid_index = df.groupby(\"location\").apply(\n",
    "    lambda x: x[\"new_cases\"].first_valid_index()\n",
    ")\n",
    "last_valid_index = df.groupby(\"location\").apply(\n",
    "    lambda x: x[\"new_cases\"].last_valid_index()\n",
    ")\n",
    "# create list of indices that we want to keep\n",
    "valid_indices = [\n",
    "    np.arange(first, last + 1)\n",
    "    for first, last in zip(first_valid_index, last_valid_index)\n",
    "]\n",
    "# flatten it to be a 1D array instead of 2D\n",
    "valid_indices = [elem for sublist in valid_indices for elem in sublist]\n",
    "df = df[df.index.isin(valid_indices)]\n",
    "# we removed rows, so we need to reset the index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at what percentage of values is still NaN for each column\n",
    "print(\"Percentage of missing values for each column\")\n",
    "(df.isna().sum() / len(df)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1085,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason total_cases is complete, but new_cases isn't so let's fix that real quick\n",
    "miss_indices = df[df[\"new_cases\"].isna()].index\n",
    "df.loc[miss_indices, \"new_cases\"] = list(\n",
    "    df.iloc[miss_indices + 1][\"total_cases\"]\n",
    "    - np.array(df.iloc[miss_indices][\"total_cases\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also it seems that there are some columns where we simply have too many missing values for them to be useful, let's remove these\n",
    "cols_to_drop = [\n",
    "    \"icu_patients\",\n",
    "    \"hosp_patients\",\n",
    "    \"weekly_icu_admissions\",\n",
    "    \"weekly_hosp_admissions\",\n",
    "    \"excess_mortality\",\n",
    "]\n",
    "df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1087,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try and fill the missing values for the remaining columns\n",
    "# vaccinations numbers are very interesting to us so let's take a look at it\n",
    "# for each country get the percentage of values that are not NaN for total_vaccinations\n",
    "print(\"Percentage of non-NaN values for each country for the column total_vaccinations\")\n",
    "(\n",
    "    df.groupby(\"location\").count()[\"total_vaccinations\"] / df.groupby(\"location\").size()\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1088,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe the high number of missing values comes from leading NaNs? let's check that\n",
    "temp = df.copy()\n",
    "temp.reset_index(inplace=True)\n",
    "first_index = temp.groupby(\"location\").apply(lambda x: x.iloc[0][\"index\"])\n",
    "first_valid_index = temp.groupby(\"location\").apply(\n",
    "    lambda x: x[\"total_vaccinations\"].first_valid_index()\n",
    ")\n",
    "print(\"Number of leading NaNs for each country for the column total_vaccinations\")\n",
    "(first_valid_index - first_index).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1089,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so it seems that there are a lot of leading NaNs, however for some countries we don't have any vaccination numbers, lets remove these countries\n",
    "temp = first_valid_index - first_index\n",
    "print(\n",
    "    \"Number of countries for which we don't have any vaccination numbers: {}\".format(\n",
    "        len(temp[temp.isna()])\n",
    "    )\n",
    ")\n",
    "# also after removing these countries, we need to reset the index (because we dropped some rows)\n",
    "df = df[~df[\"location\"].isin(temp[temp.isna()].index)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the first value of total_vaccinations for each country that isn't NaN\n",
    "first_valid_index = df.groupby(\"location\").apply(\n",
    "    lambda x: x[\"total_vaccinations\"].first_valid_index()\n",
    ")\n",
    "temp = df.iloc[list(first_valid_index)][[\"total_vaccinations\", \"location\"]].set_index(\n",
    "    \"location\"\n",
    ")\n",
    "# get the countries that have zero as first non-NaN value\n",
    "countries_with_zero = temp[temp[\"total_vaccinations\"] == 0].index\n",
    "print(\"First non-NaN value for each country for the column total_vaccinations\")\n",
    "temp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1091,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so unfortunately these aren't always zero, however just interpolating these leading NaNs would temper too much with the given data\n",
    "# let's just set the ones to zero where the first non-NaN value is zero\n",
    "df.reset_index(inplace=True)\n",
    "temp = df[df[\"location\"].isin(countries_with_zero)].groupby(\"location\")\n",
    "first_index = temp.apply(lambda x: x.iloc[0][\"index\"])\n",
    "first_valid_index = temp.apply(lambda x: x[\"total_vaccinations\"].first_valid_index())\n",
    "df.drop(columns=[\"index\"], inplace=True)\n",
    "# create list of indices that we want to change\n",
    "zero_indices = [\n",
    "    np.arange(first, last - 1) for first, last in zip(first_index, first_valid_index)\n",
    "]\n",
    "# flatten it to be a 1D array instead of 2D\n",
    "zero_indices = [elem for sublist in zero_indices for elem in sublist]\n",
    "# also set the people_vaccinated, people_fully_vaccinated, new_vaccinations to 0 for these rows\n",
    "df.loc[\n",
    "    zero_indices,\n",
    "    [\n",
    "        \"total_vaccinations\",\n",
    "        \"people_vaccinated\",\n",
    "        \"people_fully_vaccinated\",\n",
    "        \"new_vaccinations\",\n",
    "    ],\n",
    "] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1092,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the percentage of missing values for columns again\n",
    "print(\"Percentage of missing values for each column\")\n",
    "(df.isna().sum() / len(df)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1093,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's looking a lot better now, but the ~50% missing values for new_tests/total_tests are really annoying because these are such interesting columns\n",
    "# lets's check for leading NaNs\n",
    "temp = df.copy()\n",
    "temp.reset_index(inplace=True)\n",
    "# get the first index for each country\n",
    "first_index = temp.groupby(\"location\").apply(lambda x: x.iloc[0][\"index\"])\n",
    "# get the first valid index for each country for the column total_tests\n",
    "first_valid_index = temp.groupby(\"location\").apply(\n",
    "    lambda x: x[\"total_tests\"].first_valid_index()\n",
    ")\n",
    "print(\"Number of leading NaNs for each country for the column total_tests\")\n",
    "(first_valid_index - first_index).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check what the first value looks like that isn't NaN\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "first_valid_index = df.groupby(\"location\").apply(\n",
    "    lambda x: x[\"total_tests\"].first_valid_index()\n",
    ")\n",
    "first_valid_index = first_valid_index.dropna()\n",
    "temp = df.iloc[list(first_valid_index)][[\"total_tests\", \"location\"]].set_index(\n",
    "    \"location\"\n",
    ")\n",
    "print(\"First non-NaN value for each country for the column total_tests\")\n",
    "temp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1095,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately it's not zero for basically all of them, so we can't really do much here\n",
    "# maybe looking at the percentage of missing values will help us somehow\n",
    "print(\"Percentage of non-NaN values for each country for the column total_tests\")\n",
    "(\n",
    "    df.groupby(\"location\").count()[\"total_tests\"] / df.groupby(\"location\").size()\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not really sure what to do with these\n",
    "# we'll do some interpolation later for these (between first and last valid value for each country) and see to what extent that fixes it\n",
    "# let's look at other columns that have a high percentage of missing values\n",
    "print(\"Percentage of non-NaN values for each country for the column positive_rate\")\n",
    "(\n",
    "    df.groupby(\"location\").count()[\"positive_rate\"] / df.groupby(\"location\").size()\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1097,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this looks similar to total_tests (either countries have a high number of non-NaN values (> 80%) or a low number (< 20%))\n",
    "# also not sure what to do with these (we'll also interpolate it later)\n",
    "# let's take a look at another column with a high percentage of NaN-values\n",
    "print(\"Percentage of non-NaN values for each country for the column extreme_poverty\")\n",
    "(\n",
    "    df.groupby(\"location\").count()[\"extreme_poverty\"] / df.groupby(\"location\").size()\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok this is even more extreme now it's either 100% or 0% now, we can't really do anything here\n",
    "# let's take a look at another column with a high percentage of NaN-values\n",
    "print(\n",
    "    \"Percentage of non-NaN values for each country for the column handwashing_facilities\"\n",
    ")\n",
    "(\n",
    "    df.groupby(\"location\").count()[\"handwashing_facilities\"]\n",
    "    / df.groupby(\"location\").size()\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1099,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can't really do anything here as well..\n",
    "# so let's do some interpolation (we'll only interpolate between the first valid value and the last, because it would probably temper too much with the data)\n",
    "# first we'll look at the percentage of missing values for each column again\n",
    "print(\"Percentage of missing values for each column\")\n",
    "cols = df.columns\n",
    "(df.isna().sum() / len(df)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we're only gonna interpolate between the first and last valid value for each country, we can basically put each column in here and see to what extent it fixes something\n",
    "# we'll only interpolate the total_columns and add the missing values later for the new_columns\n",
    "cols_to_interpolate = [\n",
    "    \"total_deaths\",\n",
    "    \"reproduction_rate\",\n",
    "    \"total_tests\",\n",
    "    \"positive_rate\",\n",
    "    \"total_vaccinations\",\n",
    "    \"people_vaccinated\",\n",
    "    \"people_fully_vaccinated\",\n",
    "    \"stringency_index\",\n",
    "    \"population_density\",\n",
    "    \"median_age\",\n",
    "    \"aged_65_older\",\n",
    "    \"aged_70_older\",\n",
    "    \"gdp_per_capita\",\n",
    "    \"extreme_poverty\",\n",
    "    \"cardiovasc_death_rate\",\n",
    "    \"diabetes_prevalence\",\n",
    "    \"female_smokers\",\n",
    "    \"male_smokers\",\n",
    "    \"handwashing_facilities\",\n",
    "    \"hospital_beds_per_thousand\",\n",
    "    \"life_expectancy\",\n",
    "    \"human_development_index\",\n",
    "]\n",
    "df = df.groupby(\"location\").apply(\n",
    "    lambda x: x[df.columns.difference(cols_to_interpolate)].join(\n",
    "        x[cols_to_interpolate].interpolate(method=\"linear\", axis=0, limit_area=\"inside\")\n",
    "    )\n",
    ")[cols]\n",
    "# let's change new_deaths, new_tests and new_vaccinations accordingly now\n",
    "df[[\"new_deaths\", \"new_tests\", \"new_vaccinations\"]] = (\n",
    "    df.groupby(\"location\")\n",
    "    .apply(lambda x: x[[\"total_deaths\", \"total_tests\", \"total_vaccinations\"]].diff())\n",
    "    .to_numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how that affected the percentage of missing values for each column\n",
    "print(\"Percentage of missing values for each column\")\n",
    "(df.isna().sum() / len(df)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1102,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of countries that only have NaN-values for a given column\")\n",
    "(\n",
    "    df.groupby(\"location\").apply(lambda x: x.isna().all()).sum(axis=0)\n",
    "    / len(df[\"location\"].unique())\n",
    ").to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it might be fine to use the average value of the continent for a country for a specific column if it's completly missing\n",
    "# (this only applies to some columns (columns that describe local factors and that we won't expect to change much over the time interval))\n",
    "# however we should probably first look at the standard deviation and compare it with the mean and if the std is too big, we can't do it for that column\n",
    "cols_to_consider = [\n",
    "    \"location\",\n",
    "    \"population_density\",\n",
    "    \"median_age\",\n",
    "    \"aged_65_older\",\n",
    "    \"aged_70_older\",\n",
    "    \"extreme_poverty\",\n",
    "    \"cardiovasc_death_rate\",\n",
    "    \"diabetes_prevalence\",\n",
    "    \"female_smokers\",\n",
    "    \"male_smokers\",\n",
    "    \"handwashing_facilities\",\n",
    "    \"hospital_beds_per_thousand\",\n",
    "    \"life_expectancy\",\n",
    "    \"human_development_index\",\n",
    "]\n",
    "# get the continent for each country\n",
    "continents = df.groupby(\"location\").apply(lambda x: x.iloc[0][\"continent\"])\n",
    "# get the mean value for the considered columns for each country\n",
    "temp = df[cols_to_consider].groupby(\"location\").mean()\n",
    "# add continent as column\n",
    "temp[\"continent\"] = continents\n",
    "# now get the relative size of std to mean for each continent and then average that out for all continents\n",
    "means = temp.groupby(\"continent\").mean()\n",
    "means_all = (temp.groupby(\"continent\").std() / temp.groupby(\"continent\").mean()).mean()\n",
    "print(\"Relative magnitude of std compared to mean (= 1 -> mean and std are the same)\")\n",
    "means_all.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some columns the std is quite big compared to the mean value, for these columns it's probably not a good idea to just use the mean (this would temper too much with our data)\n",
    "# now we just need to define a threshhold at which we want to use the mean of the continent for missing values\n",
    "threshhold = 0.5\n",
    "cols_to_use_mean = means_all <= threshhold\n",
    "cols_to_use_mean = cols_to_use_mean[cols_to_use_mean].index\n",
    "print(\n",
    "    \"Columns to use average value of the continent for missing values with threshhold = {}:\".format(\n",
    "        threshhold\n",
    "    )\n",
    ")\n",
    "cols_to_use_mean.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean values that will be used for NaN cells\")\n",
    "means[cols_to_use_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cols_to_use_mean:\n",
    "    # get all rows that are NaN for this column\n",
    "    nan_indices = df[col].isna()\n",
    "    # set it to the mean value of the continent of that country for that column\n",
    "    df.loc[nan_indices, col] = means.loc[df[nan_indices][\"continent\"], col].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at the missing values for each column again\n",
    "print(\"Percentage of missing values for each column\")\n",
    "(df.isna().sum() / len(df)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1065,
   "metadata": {},
   "outputs": [],
   "source": [
    "# except a few columns it's looking decent now, not sure what to do further with this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Dataset: Information about policies of countries [from Oxford University](https://github.com/OxCGRT/covid-policy-tracker)\n",
    "\n",
    "### [Description of columns](https://github.com/OxCGRT/covid-policy-tracker/tree/master/documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_additional = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/OxCGRT_latest.csv\",\n",
    "    parse_dates=[\"Date\"],\n",
    ")\n",
    "df_additional.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our other dataset only has national-wide data, we'll only use it here as well\n",
    "df_additional = df_additional[df_additional[\"Jurisdiction\"] == \"NAT_TOTAL\"]\n",
    "# drop columns that were used for regional description\n",
    "df_additional.drop(columns=[\"RegionName\", \"RegionCode\", \"Jurisdiction\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentage of missing values for each column\")\n",
    "(df_additional.isna().sum() / len(df_additional)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M1_Wildcard only has missing values, lets drop that really quick\n",
    "df_additional.drop(columns=[\"M1_Wildcard\"], inplace=True)\n",
    "# we can also drop ConfirmedCases since that's complete in out other datset anyway\n",
    "# we might be able to use ConfirmedDeaths though, because there were missing values for that in our other dataset\n",
    "df_additional.drop(columns=[\"ConfirmedCases\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# except for the flag attributes this looks very good, for most of them only 2% missing values\n",
    "# E3, E4, H4 is said to not be updated anymore since August 2021 on the github repo, so that's where the relatively high number of missing values comes from\n",
    "# let's check if the missing values come from the same countries\n",
    "print(\"Number of missing values for each country for all columns\")\n",
    "df_additional.groupby(\"CountryName\").apply(lambda x: x.isna().sum().sum()).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so it's well spread between countries\n",
    "# honestly this data set just looks really good, not really sure what to do\n",
    "# since the C, E, H columns are all qualitive with very few options and the Index columns result from them, I don't think it makes sense to do interpolation on these\n",
    "# let's try interpolating missing values in ConfirmedDeaths (only between first and last valid index)\n",
    "# first sort the df it by Country and then Date\n",
    "print(\n",
    "    \"Percentage of missing values in ConfirmedDeaths before interpolation: {:.2f}%\".format(\n",
    "        (df_additional.isna().sum() / len(df_additional))\n",
    "        .to_frame()\n",
    "        .T[\"ConfirmedDeaths\"]\n",
    "        .iloc[0]\n",
    "        * 100\n",
    "    )\n",
    ")\n",
    "cols = df_additional.columns\n",
    "df_additional = df_additional.sort_values(by=[\"CountryName\", \"Date\"]).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "cols_to_interpolate = [\"ConfirmedDeaths\"]\n",
    "df_additional = df_additional.groupby(\"CountryName\").apply(\n",
    "    lambda x: x[df_additional.columns.difference(cols_to_interpolate)].join(\n",
    "        x[cols_to_interpolate].interpolate(method=\"linear\", axis=0, limit_area=\"inside\")\n",
    "    )\n",
    ")[cols]\n",
    "print(\n",
    "    \"Percentage of missing values in ConfirmedDeaths after interpolation: {:.2f}%\".format(\n",
    "        (df_additional.isna().sum() / len(df_additional))\n",
    "        .to_frame()\n",
    "        .T[\"ConfirmedDeaths\"]\n",
    "        .iloc[0]\n",
    "        * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok that didn't do anything, but it was worth a try\n",
    "# seems like it's time to merge this dataset with the other one\n",
    "# first let's check how many countries from the our dataset are present in the additional one\n",
    "temp = [\n",
    "    elem in df_additional[\"CountryName\"].unique() for elem in df[\"location\"].unique()\n",
    "]\n",
    "print(\n",
    "    \"Percentage of countries of our dataset that are also present in the additional one: {:.2f}%\".format(\n",
    "        sum(temp) / len(temp) * 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that seems high enough, so we can actually use this dataset\n",
    "# let's rename columns from the old dataset for merging\n",
    "df_additional.rename(columns={\"CountryName\": \"location\", \"Date\": \"date\"}, inplace=True)\n",
    "# we can drop redundant columns (we drop new_deaths/total_deaths, because the other dataset is more complete on these)\n",
    "df.drop(\n",
    "    columns=[\"stringency_index\", \"iso_code\", \"new_deaths\", \"total_deaths\"], inplace=True\n",
    ")\n",
    "df = df.merge(df_additional, on=[\"location\", \"date\"])\n",
    "# we have to recalculate new_deaths\n",
    "df[\"new_deaths\"] = (\n",
    "    df.groupby(\"location\").apply(lambda x: x[\"ConfirmedDeaths\"].diff()).to_numpy()\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check for percentage of missing values for each column, to see if everything worked correctly\n",
    "print(\"Percentage of missing values for each column\")\n",
    "(df.isna().sum() / len(df)).to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking good!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
